{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQSreQK-NvrB"
   },
   "source": [
    "# Image Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-chs2kuuNzHq"
   },
   "source": [
    "Cloning Data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXXUqb8XOPQX",
    "outputId": "6bf241e4-87d2-4a3e-a722-d417b2ed75e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'synthetic_datasets'...\n",
      "remote: Enumerating objects: 42, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 42 (delta 1), reused 1 (delta 1), pack-reused 39\u001b[K\n",
      "Unpacking objects: 100% (42/42), done.\n",
      "/content/synthetic_datasets/MNIST\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ayulockin/synthetic_datasets\n",
    "%cd synthetic_datasets/MNIST/\n",
    "\n",
    "### How to unzip ###\n",
    "# '%' is used before using terminal commands. Here we build dir\n",
    "# ! => shell command meant to be executed by system command line \n",
    "# -q => silently download the files without showing in output\n",
    "# -d => unzip for file inside target folder\n",
    "# !unzip -q file to unzip -d tagerted directory name if not exist it will build automatically\n",
    "\n",
    "!unzip -q MNIST_Converted_Training.zip -d image_dataset/    \n",
    "!unzip -q MNIST_Converted_Testing.zip -d image_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWw4vX7JUVh_",
    "outputId": "f19cad3a-18a9-43c8-a89f-c02b7447648f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "#  Come to starting directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yc51iwZbS_kA",
    "outputId": "9a99e1aa-bf4e-4511-eae8-9130ddc0c777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34msynthetic_datasets\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "# check in which directory u are now\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dEySKqbmDyh"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "e7haiw1vmChh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p33u71noVw61"
   },
   "source": [
    "## In this dataset we have:\n",
    "1. Simple Images of digit as MNIST_Converted_Training , MNIST_Converted_Testing.\n",
    "\n",
    "2. CSV file of test_data, train_data consist of data in tabular form. \n",
    "It consist of:\n",
    "  *   Image Directory name [which we have to modiffy little bit as per our directory saved]\n",
    "  *   Image class [Eg: 0,1,2...etc]\n",
    "  *   Bounding Box Coordinates [xmin, ymin, xmax, ymax]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SjXI_kIQTyYX"
   },
   "outputs": [],
   "source": [
    "\n",
    "S = 'synthetic_datasets'\n",
    "M = 'MNIST'\n",
    "I = 'image_dataset'\n",
    "columns_df = ['path', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "\n",
    "\n",
    "### reading Train data ###\n",
    "\n",
    "#reading csv file by passing its path location\n",
    "train_df = pd.read_csv(os.path.join(S,M, 'training_data.csv'), header=None)\n",
    "# giving column names to the train_df\n",
    "train_df.columns = columns_df\n",
    "\n",
    "\n",
    "### reading Test data ###\n",
    "\n",
    "#reading csv file by passing its path location\n",
    "test_df = pd.read_csv(os.path.join(S,M, 'test_data.csv'), header=None)\n",
    "# giving column names to the train_df\n",
    "test_df.columns = columns_df\n",
    "\n",
    "\n",
    "### Manipulation in path ###\n",
    "\n",
    "# Path of the images in traind_df and test_df is still not completly correct if we want to access the\n",
    "# image we have to struggle little bit hence adding more specific path prior to the mentioned path\n",
    "t = 'MNIST_Converted_Training'\n",
    "train_df['path'] = train_df['path'].apply(lambda s: os.path.join(S, M, I, t, s))\n",
    "\n",
    "t = 'MNIST_Converted_Testing'\n",
    "test_df['path'] = test_df['path'].apply(lambda s: os.path.join(S, M, I, t, s))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGa_kQYdjnaf"
   },
   "source": [
    "## Playing around in small scale then apply to prediction state\n",
    "\n",
    "- w'll use these two function in prediction state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "btLad9ESnL0S",
    "outputId": "9c5d7c09-9717-42ad-9e4d-964ce8a4caa7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f84fdf2a610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def load_img_vis(input_img):\n",
    "\n",
    "  # input_img = train_df['path'].iloc[0]\n",
    "\n",
    "  #reads the image using tf.io it just simply returns file as it is\n",
    "  image = tf.io.read_file(input_img)  \n",
    "\n",
    "  # image we got from previous step is the image in our dataset which is .png format now decoding it\n",
    "  image = tf.image.decode_png(image, channels=1)\n",
    "\n",
    "  # image channel was 1(gray scale) converting it to 3(bgr) AS matplot does accepts dim(x,y,1)\n",
    "  image = tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "  # tycasting of image dtype from uint8 to float32\n",
    "  image = tf.cast(image,tf.float32)\n",
    "\n",
    "  return image\n",
    "\n",
    "# calling function and plotting the image\n",
    "plt.imshow(load_img_vis(train_df['path'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cO8_MYIo5z4x",
    "outputId": "969ec5c9-3978-4d43-dea1-c2c5985ea52f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xmin    0.49\n",
       "ymin    0.15\n",
       "xmax    0.77\n",
       "ymax    0.43\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = train_df.iloc[0][2:]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "wfb253FnivOH",
    "outputId": "b135bf13-f755-4ce1-a801-2614697d95a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def add_bbox_on_img(df,index):\n",
    "\n",
    "\n",
    "  # multiply all bbox cordinates with 100 to to make it int  \n",
    "  x1, y1, x2, y2 = [int(cord*100) for cord in df.iloc[index][2:]] \n",
    "  \n",
    "  width = x2 - x1\n",
    "  height = y2 - y1\n",
    "\n",
    "  # build rectangular patch\n",
    "  rectangle = plt.Rectangle((x1,y1), width, height, fill = False, linewidth=1, edgecolor='r')\n",
    "\n",
    "  # plot patch\n",
    "  img = load_img_vis(df['path'].iloc[index])\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.imshow(img)\n",
    "  # finally plotting patch\n",
    "  ax.add_patch(rectangle)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "add_bbox_on_img(train_df,205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR0CrvAlQUoq"
   },
   "source": [
    "## Model Building:\n",
    "*   applying transfer learning using the base model as EfficientNet.\n",
    "*   Importing keras efficientNet.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PrnFbgx_-Co2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build base CNN layer EfficientNet using Keras\n",
    "def build_cnn_layer():\n",
    "  \n",
    "  # using keras.EfficientNet \n",
    "  model = tf.keras.applications.efficientnet_v2.EfficientNetV2M( \n",
    "      include_top=False,\n",
    "      weights='imagenet',\n",
    "      input_shape=(100,100,3)   )\n",
    "\n",
    "  # except last 5 layers freeze all the layers for training\n",
    "  for i in model.layers[:-5]:\n",
    "    i.trainable = False\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build Dense layers\n",
    "def build_dense_layers(input_image):\n",
    "\n",
    "  x = input_image\n",
    "  x = layers.Flatten()(x)  #Flattening of input image matrix into vector for ANN\n",
    "  x = layers.Dense(128, activation = 'relu')(x)\n",
    "  # x = layers.Dense(128, activation = 'relu')(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build classification head layer\n",
    "def build_classifier_head(input_image):\n",
    "\n",
    "  x = input_image \n",
    "  # for multi class very important to give name to head layers[class & regress], with thier name only we assign loss while compiling.\n",
    "  classifier_head = layers.Dense(10, activation = 'softmax', name = 'class')(x)   # have classify only 10 classes\n",
    "\n",
    "  return classifier_head\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build regressor head layer\n",
    "def build_regressor_head(input_image):\n",
    "  \n",
    "  x = input_image\n",
    "  regressor_head = layers.Dense(4, name = 'regressor' )(x)   # have 4 bounding box location to predict \n",
    "\n",
    "  return regressor_head\n",
    "\n",
    "\n",
    "# base_model = build_base_layer(tf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyu5mM7JCE8F"
   },
   "source": [
    "**Combining all the seperate layer we built in the seperate functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TOsytcKcYy4w"
   },
   "outputs": [],
   "source": [
    "from numpy.ma.core import shape\n",
    "\n",
    "# Combining whole architect:   [USING FUNCTIONAL API]\n",
    "\n",
    "def final_model():\n",
    "\n",
    "  image_input = tf.keras.Input(shape=(100,100,3))  # passing input shape necessary in functional API\n",
    "\n",
    "  #adding cnn layer\n",
    "  base_model = build_cnn_layer() #build varaible to store the function returned value\n",
    "  x = base_model(image_input)  #input the data in base_model using functional API mtd\n",
    "\n",
    "  # adding dense layer\n",
    "  x = build_dense_layers(x)\n",
    "\n",
    "### Two Output heads needs the same input x from the dense layer ###\n",
    "\n",
    "  #adding classif head\n",
    "  classif_head = build_classifier_head(x)\n",
    "\n",
    "  #adding regressor head\n",
    "  regress_head = build_regressor_head(x)\n",
    "\n",
    "\n",
    "  # multi-output hence passing list of output heads in output parameter\n",
    "  return  tf.keras.Model(inputs= image_input, outputs= [classif_head, regress_head])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWScvLFb9Omp"
   },
   "source": [
    "## Arranging Image data to pass for model training:\n",
    " - We have image path in train_df & test_df.\n",
    " - So to give the image to the model we first have to take the image from the path we cannot dirctly give the path to the model bcz model accepts image as tensor/matrix not the string of path.\n",
    " - Hence w'll create load_image() function.\n",
    " - the above function will read the path -> decode it -> convert the channel -> change value dtype from uint8 to float.\n",
    " - Also we have to create tensorflow dataset to store all this Image(not path) + class + bbox locations **bcz model accepts tensorflow dataset not .csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "FgqefQ7K4XKx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columns_df = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "\n",
    "# converting all df_train columns into individual arrays and passing it to form tensorflow dataset\n",
    "# train_path_arr = np.array(train_df['path'])\n",
    "train_class_arr = np.array(train_df['class'])\n",
    "train_bbox_arr = np.array(train_df[columns_df])\n",
    "\n",
    "# converting all df_test columns into individual arrays and passing it to form tensorflow dataset\n",
    "# test_path_arr = np.array(test_df['path'])\n",
    "test_class_arr = np.array(test_df['class'])\n",
    "test_bbox_arr = np.array(test_df[columns_df])\n",
    "\n",
    "\n",
    "#### NOTE: ####\n",
    "#1. from_tensor_slices is used to build tensorflow dataset bcz we cannot pass train_df as dataset to model bcz it is in .csv\n",
    "#2. we need to pass two parameter to from_tensor_slice as bcz load_image fuinction takes two argumen(image path & lables)\n",
    "#3. Although labels should be just class name but we are now solving Multi-lassification problem [class & regression]\n",
    "#4. Make sure in tensorflow dataset for multi-class u pass the dictonary key name same as two head layers name bcz this will\n",
    "#   help during compiling the model while passing the trainind and testing dataset.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['path'].tolist(), \n",
    "                                                     {\"class\":train_class_arr, \"regressor\":train_bbox_arr}) )\n",
    " \n",
    "\n",
    "# from_tensor_slices is used to build tensorflow dtype dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['path'].tolist(), \n",
    "                                                    {\"class\":test_class_arr, \"regressor\":test_bbox_arr}) )\n",
    "\n",
    "\n",
    "# process of getting the image through the path and return image as matrix & lable dict of {class, regressor} as it is\n",
    "def load_image(path, label):\n",
    "\n",
    "  # read the file from the path\n",
    "  image = tf.io.read_file(path)\n",
    "\n",
    "  #now to decode the file from previous readfile because it read the file and stored it in Machine language now we have to decode it \n",
    "  image = tf.image.decode_png(image, channels=1)\n",
    "  \n",
    "  # # convert the channels from 1 to 3 bcz model accepts 3 channel\n",
    "  image = tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "  image = tf.cast(image, dtype=tf.float32) / 255. #after typecast do normalize also\n",
    "\n",
    "  # image = tf.expand_dims(image, axis=0)\n",
    "  # image = tf.expand_dims(image, axis=0)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "# Now to successfully replace the train_df path containing part with actuall image matrix as well as with dictonary\n",
    "# of class and regresso value, all this inside tensorflow dataset.\n",
    "\n",
    "# using map() with tensorflow dataset [tf_Dataset.map(func)]  --> tf_dataset is like iterable ---> see documentation\n",
    "train_dataset = train_dataset.map(load_image).batch(30)  #randomly assigned batch size, \n",
    "\n",
    "test_dataset = test_dataset.map(load_image).batch(30)   #randomly assigned batch size\n",
    "\n",
    "\n",
    "#### For understanding try run this loop what's inside train_dataset ####\n",
    "# for i in train_dataset:\n",
    "#   print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rsysJN6WdPY"
   },
   "source": [
    "## Model Compilation:\n",
    " - w'll use tensorflow compile method which is a function of tf.keras.model.\n",
    " - **we won't be using a custom loops for training the model. custom loop give as more flexibilty in our code for training the model watch https://www.youtube.com/watch?v=q7ZuZ8ZOErE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGBAmsWnWdrA",
    "outputId": "0997249b-19c5-4933-b6f6-30ae5488153c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetv2-m (Functional)  (None, 4, 4, 1280)   53150388    ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 20480)        0           ['efficientnetv2-m[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          2621568     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " class (Dense)                  (None, 10)           1290        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " regressor (Dense)              (None, 4)            516         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,773,762\n",
      "Trainable params: 3,281,294\n",
      "Non-trainable params: 52,492,468\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# calling the final_model()\n",
    "model = final_model()\n",
    "\n",
    "model.summary()  #checking if everything is okay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNLpM6JI9Lk_",
    "outputId": "9c0e66f4-443f-4e44-e7b7-b8b1aa90db0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetv2-m (Functional)  (None, 4, 4, 1280)   53150388    ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 20480)        0           ['efficientnetv2-m[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          2621568     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " class (Dense)                  (None, 10)           1290        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " regressor (Dense)              (None, 4)            516         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,773,762\n",
      "Trainable params: 3,281,294\n",
      "Non-trainable params: 52,492,468\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/4\n",
      "   1/2000 [..............................] - ETA: 9:14:30 - loss: 2.3291 - class_loss: 2.2862 - regressor_loss: 0.0429 - class_accuracy: 0.1333 - regressor_mse: 0.0429"
     ]
    }
   ],
   "source": [
    "#compile\n",
    "\n",
    "model.compile( optimizer = 'adam',\n",
    "              loss = {'class':tf.keras.losses.SparseCategoricalCrossentropy(), 'regressor': tf.keras.losses.MeanSquaredError()},\n",
    "              metrics = {'class':['accuracy'], 'regressor': ['mse']} )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_dataset, validation_data= test_dataset, batch_size=30, epochs = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnFguYBuiKMX"
   },
   "source": [
    "## Save Model and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "F90pXUPEgwj3"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "# model.save('CNN_04_model')\n",
    "\n",
    "# load model back\n",
    "model = tf.keras.models.load_model('CNN_04_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOqZD2POlIhE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T87D208FjIV1"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "7ytEzPs_mbue"
   },
   "outputs": [],
   "source": [
    "def load_img_vis(input_img):\n",
    "\n",
    "  # input_img = train_df['path'].iloc[0]\n",
    "\n",
    "  #reads the image using tf.io it just simply returns file as it is\n",
    "  image = tf.io.read_file(input_img)  \n",
    "\n",
    "  # image we got from previous step is the image in our dataset which is .png format now decoding it\n",
    "  image = tf.image.decode_png(image, channels=1)\n",
    "\n",
    "  # image channel was 1(gray scale) converting it to 3(bgr) AS matplot does accepts dim(x,y,1)\n",
    "  image = tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "  # tycasting of image dtype from uint8 to float32\n",
    "  image = tf.cast(image,tf.float32)\n",
    "\n",
    "  \n",
    "  image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "4ZsE5Ta3nIgT",
    "outputId": "a27792af-d92d-4bef-9d6b-d2adfafb3720"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 3)\n",
      "(100, 100, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8458516ca0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calling the load_img_vis() to get the image from test data and passing for prediction\n",
    "img = load_img_vis(test_df['path'].iloc[1])\n",
    "print(img.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = tf.squeeze(img, axis=0)\n",
    "print(img.shape)\n",
    "ax.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KToix9PmmuP",
    "outputId": "b2420565-aab7-4da3-d158-e88175f58693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "max probability class: 1\n",
      "bbox coordinates: [0.3550005555152893, 0.3625701665878296, 0.634964644908905, 0.6424840688705444]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prediction(img):\n",
    "  \n",
    "  # model.predict it return array of image_class[10] + regression[4] for each inserted image\n",
    "  predict = model.predict(img)\n",
    "\n",
    "\n",
    "  # predicted class of image\n",
    "  class_predict = np.argmax(predict[0][0].tolist())  #max probability class\n",
    "  print('max probability class:', class_predict)\n",
    "\n",
    "\n",
    "  # predicted bbox location of image\n",
    "  reg_predict = predict[1][0].tolist()\n",
    "  print('bbox coordinates:', reg_predict)\n",
    "\n",
    "  return class_predict, reg_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "Zs3eQ12DWN_i",
    "outputId": "e933deb1-dc25-4e15-9403-8de390100b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max probability class: 1\n",
      "bbox coordinates: [0.3550005555152893, 0.3625701665878296, 0.634964644908905, 0.6424840688705444]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_bbox_on_img(df,index):\n",
    "\n",
    "  # calling the load_img_vis() to get the image from test data and passing for prediction\n",
    "  img = load_img_vis(df['path'].iloc[index])\n",
    "\n",
    "  # calling prediction() to use its value here for plotting image with bbox\n",
    "  class_predict, reg_predict = prediction(img)\n",
    "\n",
    "\n",
    "  # multiply all bbox cordinates with 100 to to make it int  \n",
    "  x1, y1, x2, y2 = [int(cord*100) for cord in reg_predict] \n",
    "  \n",
    "  width = x2 - x1\n",
    "  height = y2 - y1\n",
    "\n",
    "  # build rectangular patch\n",
    "  rectangle = plt.Rectangle((x1,y1), width, height, fill = False, linewidth=1, edgecolor='r')\n",
    "\n",
    "  # plot patch\n",
    "  img = tf.squeeze(img, axis=0) #reducing dimension if image from 4 to 3 bcz plt accepts 3 dim\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.imshow(img)\n",
    "  # finally plotting patch\n",
    "  ax.add_patch(rectangle)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "add_bbox_on_img(train_df, 5972)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKZa1M-Gseda"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gGa_kQYdjnaf"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
